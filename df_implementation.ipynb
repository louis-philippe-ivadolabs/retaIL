{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e9632-2e04-4d11-a87a-83c6e7898c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandera as pa\n",
    "from abc import ABC\n",
    "from pathlib import Path\n",
    "from enum import Enum, auto\n",
    "from typing import Callable, List\n",
    "\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "#To be imported if needed\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca3a16-d0d3-480f-8467-ea274ea6871d",
   "metadata": {},
   "source": [
    "### Generic components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d61d52-7e28-4b60-984a-591165f1fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature:\n",
    "    def __init__(self, \n",
    "                 name: str, \n",
    "                 is_categorical: bool,\n",
    "                 aggregation_f: Callable,\n",
    "                 pa_check: pa.Check = None,\n",
    "                 transformations: List[Callable] = None):\n",
    "        self.name = name\n",
    "        self.is_categorical = is_categorical\n",
    "        self.aggregation_f = aggregation_f\n",
    "        self.pa_check = pa_check\n",
    "        self.transformations = transformations\n",
    "\n",
    "\n",
    "class DFTransformer(ABC):\n",
    "    def transform(self, \n",
    "                  df: pd.DataFrame,\n",
    "                  store_interim_dfs=False) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "\n",
    "AggregationLevel = {\n",
    "    \"transaction_date\": ['store_id', 'sku_id', 'transaction_date'],\n",
    "    \"year_week\": ['store_id', 'sku_id', 'year_week'],\n",
    "    \"year\": ['store_id', 'sku_id', 'year']\n",
    "}\n",
    "\n",
    "\n",
    "class Aggregator(DFTransformer):\n",
    "    def __init__(self, \n",
    "                 aggregation_level: AggregationLevel,\n",
    "                 feature_list: List[Feature],\n",
    "                 logger: logging.Logger = None):\n",
    "        self._aggregation_level = aggregation_level\n",
    "        self._feature_list = feature_list\n",
    "        self._logger = logger or logging.getLogger(__name__)\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        original_n_rows = df.shape[0]\n",
    "        feature_aggregation_f_dict = dict(\n",
    "                                          [(f.name, f.aggregation_f) \n",
    "                                           for f in self._feature_list\n",
    "                                           if f.name not in self._aggregation_level\n",
    "                                          ]\n",
    "                                         )\n",
    "        df = df.groupby(self._aggregation_level).agg(feature_aggregation_f_dict).reset_index()\n",
    "        self._logger.info(f\"Aggregator aggregated {original_n_rows} rows into {df.shape[0]}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "class Scoper(DFTransformer):\n",
    "    def __init__(self, \n",
    "                 start_date: str = None, \n",
    "                 end_date: str = None, \n",
    "                 store_id_list: List[str] = None,\n",
    "                 cat_id_list: List[str] = None ,\n",
    "                 sku_id_list: List[str] = None,\n",
    "                 dept_id_list: List[str] = None,\n",
    "                 logger: logging.Logger = None\n",
    "                ):\n",
    "        self._start_date = start_date\n",
    "        self._end_date = end_date\n",
    "        self._store_id_list = store_id_list\n",
    "        self._cat_id_list = cat_id_list\n",
    "        self._sku_id_list = sku_id_list\n",
    "        self._dept_id_list = dept_id_list\n",
    "        self._logger = logger or logging.getLogger(__name__)\n",
    "\n",
    "    def transform(self, \n",
    "                 df: pd.DataFrame, \n",
    "                ) -> pd.DataFrame:\n",
    "        original_n_rows = df.shape[0]\n",
    "        if self._start_date:\n",
    "            df = df[df[\"transaction_date\"] >= self._start_date]\n",
    "        if self._end_date:\n",
    "            df = df[df[\"transaction_date\"] < self._end_date]\n",
    "        if self._store_id_list:\n",
    "            df = df[df[\"store_id\"].isin(self._store_id_list)]\n",
    "        if self._cat_id_list:\n",
    "            df = df[df[\"cat_id\"].isin(self._cat_id_list)]\n",
    "        if self._sku_id_list:\n",
    "            df = df[df[\"sku_id\"].isin(self._sku_id_list)]\n",
    "        if self._dept_id_list:\n",
    "            df = df[df[\"dept_id\"].isin(self._dept_id_list)]\n",
    "        self._logger.info(f\"Scoper removed {original_n_rows - df.shape[0]} Rows. Remaining number of rows: {df.shape[0]}\")\n",
    "        return df\n",
    "        \n",
    "\n",
    "class FeatureSelector(DFTransformer):\n",
    "    def __init__(self, \n",
    "                 feature_list: List[Feature],\n",
    "                 logger: logging.Logger = None\n",
    "                ):\n",
    "        self._feature_list = feature_list\n",
    "        self._logger = logger or logging.getLogger(__name__)\n",
    "\n",
    "    def transform(self, \n",
    "                  df: pd.DataFrame, \n",
    "                 ) -> pd.DataFrame:\n",
    "        original_n_features = df.shape[1]\n",
    "        df = df[[f.name for f in self._feature_list]]\n",
    "        self._logger.info(f\"Feature selector removed {original_n_features - df.shape[1]} features, keeping {df.shape[1]}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DemandPredictor(ABC):\n",
    "    def fit(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, data):\n",
    "        pass\n",
    "\n",
    "\n",
    "class CatBoostDemandPredictor(DemandPredictor):\n",
    "    def fit(self, \n",
    "            train_df: pd.DataFrame,\n",
    "            target: pd.Series,\n",
    "            feature_list: List[Feature]\n",
    "           ):\n",
    "        cat_features = [f.name for f in feature_list if f.is_categorical]\n",
    "        print('cat features: ', cat_features)\n",
    "        self._model = CatBoostRegressor(cat_features=cat_features)\n",
    "        self._model.fit(train_df, target)\n",
    "\n",
    "    def predict(self, prediction_df: pd.DataFrame):\n",
    "        return self._model.predict(prediction_df)\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,\n",
    "                 feature_list: List[Feature],\n",
    "                 demand_predictor: DemandPredictor,\n",
    "                 transformer_list: List[DFTransformer],\n",
    "                 logger: logging.Logger = None\n",
    "                ):\n",
    "        self._feature_list = feature_list\n",
    "        self._demand_predictor = demand_predictor\n",
    "        self._transformer_list = transformer_list\n",
    "        self._logger = logger or logging.getLogger(__name__)\n",
    "\n",
    "    def preprocess_sales_df(self, sales_df: pd.DataFrame):\n",
    "        self._logger.info(f\"Preprocessing the data. Data shape: {sales_df.shape}\")\n",
    "        for transformer in self._transformer_list:\n",
    "            sales_df = transformer.transform(sales_df)\n",
    "        self._logger.info(f\"Data shape after preprocessing: {sales_df.shape}\")\n",
    "        return sales_df\n",
    "\n",
    "    def fit(self, preprocessed_sales_df: pd.DataFrame):\n",
    "        print(preprocessed_sales_df.head())\n",
    "        self._demand_predictor.fit(preprocessed_sales_df.drop(\"sales_qty\", axis=1), \n",
    "                                   preprocessed_sales_df[\"sales_qty\"], \n",
    "                                   self._feature_list)\n",
    "        \n",
    "    def predict(self, pred_df: pd.DataFrame):\n",
    "        return self._demand_predictor.predict(pred_df)\n",
    "\n",
    "\n",
    "class ModelBuilder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def with_demand_predictor(self, demand_predictor: DemandPredictor):\n",
    "        self._demand_predictor = demand_predictor\n",
    "        return self\n",
    "\n",
    "    def with_feature_list(self, feature_list: List[Feature]):\n",
    "        self._feature_list = feature_list\n",
    "        return self\n",
    "\n",
    "    def with_transformer_list(self, transformer_list: List[DFTransformer]):\n",
    "        self._transformer_list = transformer_list\n",
    "        return self\n",
    "\n",
    "    def build(self):\n",
    "        return Model(\n",
    "            self._feature_list,\n",
    "            self._demand_predictor,\n",
    "            self._transformer_list\n",
    "        )\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model: Model):\n",
    "        self._model = model\n",
    "\n",
    "    def mae(self, \n",
    "            demand_predictions: pd.Series,\n",
    "            demand_actuals: pd.Series):\n",
    "        return np.mean(np.abs(demand_predictions - demand_actuals))\n",
    "\n",
    "    def __call__(self, preprocessed_prediction_df: pd.DataFrame):\n",
    "        demand_predictions = self._model.predict(preprocessed_prediction_df.drop('sales_qty', axis=1))\n",
    "        demand_actuals = preprocessed_prediction_df['sales_qty']\n",
    "        return {\n",
    "                 \"mae\": self.mae(demand_predictions, demand_actuals) \n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbcadd3-c398-4cae-8520-e93d2958281e",
   "metadata": {},
   "source": [
    "### Dataset-specific components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafab897-f7e0-4ddb-a0b4-88e6ed903f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(ABC):\n",
    "    def _load():\n",
    "        pass\n",
    "\n",
    "\n",
    "class M5DataLoader(DataLoader):\n",
    "    def __init__(self, root_dir: Path):\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def get_sales_df(self, sales_file: str) -> pd.DataFrame:\n",
    "        self._load_raw_sales(\n",
    "                             sales_file\n",
    "                            )._add_calendar(\n",
    "                            )._add_prices(\n",
    "                            )._conform(\n",
    "                            )\n",
    "        return self.sales_df\n",
    "\n",
    "    def _load_raw_sales(self, sales_file: str):\n",
    "        key_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "        self.sales_df = pd.read_csv(self.root_dir / sales_file)\n",
    "        self.sales_df = self.sales_df.melt(id_vars=key_columns, var_name='d',value_name='sales_qty')    \n",
    "        return self\n",
    "\n",
    "    def _add_calendar(self):\n",
    "        self.calendar_df = pd.read_csv(self.root_dir / \"calendar.csv\")\n",
    "        calendar_df = self.calendar_df[['date','d', 'wm_yr_wk']]\n",
    "        self.sales_df = pd.merge(self.sales_df, self.calendar_df, on='d')\n",
    "        return self\n",
    "\n",
    "    def _add_prices(self):\n",
    "        self.prices_df = pd.read_csv(self.root_dir / \"sell_prices.csv\")\n",
    "        self.sales_df = pd.merge(self.sales_df, self.prices_df, on=['store_id', 'item_id', 'wm_yr_wk'])\n",
    "        return self\n",
    "\n",
    "    def _conform(self):\n",
    "        self.sales_df = self.sales_df.rename(columns={\n",
    "                                                \"item_id\": \"sku_id\", \n",
    "                                                \"wm_yr_wk\": \"year_week\",\n",
    "                                                \"date\": \"transaction_date\",\n",
    "                                                \"sell_price\": \"effective_price\"\n",
    "                                              }\n",
    "                                            )\n",
    "        self.sales_df.sales_qty = self.sales_df.sales_qty.astype(float)\n",
    "        self.sales_df.year_week = self.sales_df.year_week.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f247200-21f0-42e3-a785-a39acc89d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO ITEMS: \n",
    "\n",
    "### Adding arbitrary features (features must come with pandera check + aggregation function + is_cat + transformations(?))\n",
    "### Define a non-strict pandera contract\n",
    "### Validate input\n",
    "### Define aggregation-level\n",
    "### train\n",
    "### Add metrics\n",
    "# use groupby columns to merge back to the original set of features\n",
    "# Define standard featurizers\n",
    "# Define/Apply general scope\n",
    "# Scope and Feature to be extendable\n",
    "# GM: All preprocess steps to be feature engineering steps. User to choose order\n",
    "# GM: Define custom preproccesor (possibly using different data sources)\n",
    "# LP: Add offered articles as contract (infer from sales, or another data source)\n",
    "# GM: Elasticity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053e4ac9-023e-4538-9ead-2a961f1d1e90",
   "metadata": {},
   "source": [
    "### Non-strict (extendable) input contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f37934c-8d1d-4cf1-b26b-2834d325c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_schema = pa.DataFrameSchema({\n",
    "    \"effective_price\": pa.Column(float, checks=pa.Check.ge(0.0)),\n",
    "    \"sales_qty\": pa.Column(float, checks=pa.Check.ge(0.0)),\n",
    "    \"sku_id\": pa.Column(str),\n",
    "    \"store_id\": pa.Column(str),\n",
    "    \"dept_id\": pa.Column(str),\n",
    "    \"cat_id\": pa.Column(str),\n",
    "    \"transaction_date\": pa.Column(str),\n",
    "    \"year\": pa.Column(int, checks=pa.Check.gt(1900)),\n",
    "    \"month\": pa.Column(int, checks=(pa.Check.ge(1), pa.Check.le(12))),\n",
    "    \"year_week\": pa.Column(str)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760c16b-1d6d-49e6-8a60-05ca99bcddd4",
   "metadata": {},
   "source": [
    "## Usage example\n",
    "#### First step: compose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7a302-702b-4a60-bf6f-8510d1dc930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create feature list\n",
    "feature_list = [\n",
    "                Feature('effective_price', False, 'mean'),\n",
    "                Feature('sales_qty', False, 'sum'),\n",
    "                Feature('store_id', True, 'first'),\n",
    "                Feature('year_week', False, 'first'),\n",
    "                Feature('sku_id', True, 'first')\n",
    "               ]\n",
    "\n",
    "demand_predictor = CatBoostDemandPredictor()\n",
    "\n",
    "scoper = Scoper(\n",
    "    start_date='2012-01-01',\n",
    "    end_date='2014-01-01',\n",
    "    cat_id_list=[\"HOBBIES\"]\n",
    ")\n",
    "\n",
    "feature_selector = FeatureSelector(feature_list)\n",
    "\n",
    "aggregator = Aggregator(AggregationLevel[\"year_week\"], feature_list)\n",
    "\n",
    "model = ModelBuilder(\n",
    "    ).with_demand_predictor(\n",
    "        demand_predictor\n",
    "    ).with_feature_list(\n",
    "        feature_list\n",
    "    ).with_transformer_list(\n",
    "        [\n",
    "            scoper,\n",
    "            aggregator,\n",
    "            feature_selector\n",
    "        ]\n",
    "    ).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a855ec-e05b-494d-b8ba-9b8b8e8ba1be",
   "metadata": {},
   "source": [
    "#### Second step: fit and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc2c7ce-fde0-4097-a61e-7123d988e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = M5DataLoader(root_dir=Path(\"/Users/mehditowhidi/Downloads/m5-forecasting-accuracy\")).get_sales_df(\"sales_train_evaluation.csv\")\n",
    "df = model_input_schema.validate(df)\n",
    "preprocessed_train_df = model.preprocess_sales_df(df)\n",
    "\n",
    "model.fit(preprocessed_train_df)\n",
    "\n",
    "val_df = M5DataLoader(root_dir=Path(\"/Users/mehditowhidi/Downloads/m5-forecasting-accuracy\")).get_sales_df(\"sales_train_validation.csv\")\n",
    "val_df = model_input_schema.validate(val_df)\n",
    "preprocessed_val_df = model.preprocess_sales_df(val_df)\n",
    "\n",
    "Evaluator(model)(preprocessed_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edad86f5-39da-430c-9f94-b10cc86d9bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
